---
kind: pipeline
name: default

steps:
  ###################
  # DEVELOPMENT
  ###################
  - name: deploy_composer_dev
    image: us.gcr.io/nyt-registry-prd/drone-gcc:latest
    settings:
      env_name: mb-composer-dev
      location: us-east1
      zone: us-east1-b
      node_type: n1-standard-4
      node_count: 3
      network: nyt-mbcompdev-dev-net
      sub_network: nyt-mbcompdev
      image_version: composer-1.15.0-airflow-1.10.14
      service_account: drone-gcc-dev-754@nyt-mbcompdev-dev.iam.gserviceaccount.com
      # Remove the below entry if you don't have any additional python packages to install.
      # Otherwise the deployment will fail when it doesn't find the install.txt file.
      deploy_strategy:
         - dags : dags
#         - plugins : plugins
      airflow_configs:
         - core-dags_are_paused_at_creation : False
         - core-max_active_runs_per_dag : 3
         - webserver-dag_default_view : tree
      env_vars:
         - SENDGRID_MAIL_FROM : mb_gcc_drone_dev@nytimes.com
         - SENDGRID_API_KEY : $$V$$SENDGRID_API_KEY_DEV
      airflow_vars:
           ENV : DEV
      connections:
         - conn_id : bigquery_default
           conn_type : google_cloud_platform
           conn_extra: '{"extra__google_cloud_platform__project": "nyt-mbcompdev-dev", "extra__google_cloud_platform__key_path": "$$P$$DEV_CREDENTIALS_FOR_RUNNING_AIRFLOW_DAGS" ,"extra__google_cloud_platform__scope": "https://www.googleapis.com/auth/bigquery,https://www.googleapis.com/auth/cloud-platform"}'
      # These kind of secrets are needed by containers launched by a KubernetesPodOperator.  A container launched by a KubernetesPodOperator won't
      # be able to access the secrets set to environment variables specified in the 'env_vars' section above.  Only the regular Airflow operators run by an Airflow
      # worker will be able to access those environment variables.  Instead, this will create a kubernetes secret object which can then be fed to a KubernetesPodOperator as
      # a parameter so that the container can also access the secrets it needs.
#      kubernetes_secrets:
#        SOME_KUBE_SECRET_1: $$V$$SECRET_1_DEV
#        SOME_KUBE_SECRET_2: $$V$$SECRET_2_DEV
    # This is where drone variables can be set equal to secrets pulled from drone secrets. Those drone variables that can then in turn
    # be used in the 'env_vars' section above to make the secrets available in Cloud Composer via environment variables the Airflow workers can access.
    environment:
      TOKEN:
        from_secret: drone-gcc-secret-dev
      DEV_CREDENTIALS_FOR_RUNNING_AIRFLOW_DAGS:
        from_secret: dev_credentials_for_running_airflow_dags
      SENDGRID_API_KEY_DEV:
        from_secret: send_grid_api_key

  - name: insert_airflow_vars_from_json
    image: python:3.8.0
    commands:
      - echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list
      - apt-get install apt-transport-https ca-certificates gnupg
      - curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key --keyring /usr/share/keyrings/cloud.google.gpg add -
      - apt-get update && apt-get install google-cloud-sdk -y -V
      - python3 ./create_json.py
      - gcloud auth activate-service-account --key-file=DRONE_GCC_SECRET_DEV.json --project=nyt-mbcompdev-dev
      - gcloud composer environments storage data import --source=variable.json --environment=mb-composer-dev --location=us-east1
      - gcloud composer environments run mb-composer-dev --location=us-east1 variables -- --i /home/airflow/gcs/data/variable.json
      - rm DRONE_GCC_SECRET_DEV.json
    environment:
      DRONE_GCC_SECRET_DEV:
        from_secret: drone-gcc-secret-dev
    when:
      event: push
      branch:
        - dev
